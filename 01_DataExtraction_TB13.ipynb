{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from office365.runtime.auth.authentication_context import AuthenticationContext\n",
    "from office365.sharepoint.client_context import ClientContext\n",
    "import pandas as pd\n",
    "\n",
    "from influxdb import InfluxDBClient\n",
    "from functools import reduce\n",
    "\n",
    "import dcor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Constants for the SharePoint connection\n",
    "# TENANT_ID = \"c9dc265f-a55d-466a-920c-9eb2e81f750f\"\n",
    "# N2_URL = \"https://n2applied.sharepoint.com/sites/\"\n",
    "# APPLICATION_ID = \"f83dae31-1ffa-4457-8e00-bc48c0f02a29\"\n",
    "\n",
    "# def authenticate_sharepoint(site_name: str):\n",
    "#     \"\"\"Authenticate and return a client context for a SharePoint site.\"\"\"\n",
    "#     site_url = f'{N2_URL}{site_name}'\n",
    "#     ctx_auth = AuthenticationContext(url=site_url)\n",
    "#     ctx_auth.with_interactive(TENANT_ID, APPLICATION_ID)\n",
    "#     ctx = ClientContext(site_url, ctx_auth)\n",
    "#     return ctx\n",
    "\n",
    "# def download_file(ctx: ClientContext, folder_path: str, file_name: str):\n",
    "#     \"\"\"Download a file from SharePoint.\"\"\"\n",
    "#     file_path = f'{folder_path}/{file_name}'\n",
    "#     with open(file_name, \"wb\") as file:\n",
    "#         ctx.web.get_file_by_server_relative_url(file_path).download(file).execute_query()\n",
    "\n",
    "# def load_excelsheet(ctx: ClientContext, folder_path: str, file_name: str, sheet_name: str) -> pd.DataFrame:\n",
    "#     \"\"\"Load an Excel sheet from SharePoint into a pandas DataFrame.\"\"\"\n",
    "#     file_path = f'{folder_path}/{file_name}'\n",
    "#     excel_file = ctx.web.get_file_by_server_relative_url(file_path)\n",
    "#     file_content = excel_file.get_content().execute_query()\n",
    "#     excel_sheet = pd.read_excel(io=BytesIO(file_content.value), sheet_name=sheet_name, skiprows=8)\n",
    "#     return excel_sheet\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Fill in these variables with your specific values\n",
    "#     site_name = 'RD'  # Replace with your actual site name\n",
    "#     folder_path = \"Projects/N2-PNG Stability and qualification\"  # Replace with your actual folder path\n",
    "#     file_name = \"2024-1068-Testplan and log.xlsx\"  # Replace with your actual file name\n",
    "#     sheet_name = '2024-1018-TestLog'  # Replace with the sheet name you want\n",
    "#     # Authenticate and obtain client context\n",
    "#     ctx = authenticate_sharepoint(site_name)\n",
    "#     # Download the file\n",
    "#     download_file(ctx, folder_path, file_name)\n",
    "#     # Load the Excel sheet into a DataFrame\n",
    "#     excel_data = load_excelsheet(ctx, folder_path, file_name, sheet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data = pd.read_excel('2024-1068-Testplan and log.xlsx','2024-1018-TestLog', skiprows=8)\n",
    "# excel_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify TB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_number = 13\n",
    "lt = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Influx data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename labels\n",
    "metadata_df = excel_data.rename(columns={'DSE material loss [g/h]':'material_loss_rate'})\n",
    "metadata_df.columns\n",
    "## Filter Excel Data\n",
    "metadata_df = metadata_df[['Test_ID', 'Run Order', 'TB', 'Start Date [dd.mm.yyyy]',\n",
    "       'Exp start Time [hh:mm]', 'Stop date\\n [dd.mm.yyy]',\n",
    "       'Exp stop Time [hh:mm]', 'Duration [h]', 'Quench diameter\\n[mm]',\n",
    "       'Quench distance\\n[mm]', 'Running mode', 'Target Power [kW]',\n",
    "       'Target current [A]', 'Target voltage [V]', 'Target aiflow [m3/h]',\n",
    "       'Swirlinator type', 'Swirlinator inlets', 'Inlet diameter [mm]',\n",
    "       'Coolant temp [C]', 'DSE ', 'DSE type', 'USE #',\n",
    "       'DSE weight before [g]', 'DSE weight after [g]', 'material_loss_rate',\n",
    "       'DSE material loss [g]', 'USE depth before [mm]',\n",
    "       'USE depth after [mm]', 'USE material loss [mm/h]']].copy()\n",
    "\n",
    "metadata_df['DSE material loss [g/h]'] = pd.to_numeric(metadata_df['material_loss_rate'], errors='coerce')\n",
    "\n",
    "# dropped non valid rows\n",
    "metadata_df.dropna(subset=['Test_ID',\n",
    "                           'Exp start Time [hh:mm]',\n",
    "                           'Start Date [dd.mm.yyyy]',\n",
    "                           'Exp start Time [hh:mm]',\n",
    "                           'Stop date\\n [dd.mm.yyy]',\n",
    "                           'Exp stop Time [hh:mm]',\n",
    "                           'DSE material loss [g/h]'\n",
    "                           ], inplace=True)\n",
    "\n",
    "# Only include rows where Test OK = 1\n",
    "# excel_data = excel_data.query('`Test OK 1=OK` == 1')\n",
    "# excel_data = excel_data.query('`Test OK 1=OK` != 1')\n",
    "\n",
    "metadata_df = metadata_df.query('`DSE ` == [\"B52#1\",\"B52#2\",\"B52#3\"]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin Henry Chen\\AppData\\Local\\Temp\\ipykernel_26764\\3801585021.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  metadata_df['exp_start'] = pd.to_datetime(metadata_df['Start Date [dd.mm.yyyy]'].astype(str) + ' ' + metadata_df['Exp start Time [hh:mm]'].astype(str), errors='coerce').dt.tz_localize('Europe/Oslo')\n",
      "C:\\Users\\Admin Henry Chen\\AppData\\Local\\Temp\\ipykernel_26764\\3801585021.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  metadata_df['exp_stop'] = pd.to_datetime(metadata_df['Stop date\\n [dd.mm.yyy]'].astype(str) + ' ' + metadata_df['Exp stop Time [hh:mm]'].astype(str), errors='coerce').dt.tz_localize('Europe/Oslo')\n"
     ]
    }
   ],
   "source": [
    "# Convert columns to datetime and catch errors\n",
    "metadata_df['exp_start'] = pd.to_datetime(metadata_df['Start Date [dd.mm.yyyy]'].astype(str) + ' ' + metadata_df['Exp start Time [hh:mm]'].astype(str), errors='coerce').dt.tz_localize('Europe/Oslo')\n",
    "metadata_df['exp_stop'] = pd.to_datetime(metadata_df['Stop date\\n [dd.mm.yyy]'].astype(str) + ' ' + metadata_df['Exp stop Time [hh:mm]'].astype(str), errors='coerce').dt.tz_localize('Europe/Oslo')\n",
    "\n",
    "# Drop rows where datetime conversion failed (NaT values)\n",
    "metadata_df = metadata_df.dropna(subset=['exp_start', 'exp_stop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "antall exp = 5\n",
      "excluded = []\n"
     ]
    }
   ],
   "source": [
    "## Removing specific datapoints\n",
    "metadata_df = metadata_df.query(f'TB == {tb_number}')\n",
    "test_ids_to_exclude = []\n",
    "metadata_df = metadata_df[~metadata_df['Test_ID'].isin(test_ids_to_exclude)]\n",
    "metadata_df_copy = metadata_df.set_index('Test_ID').copy()\n",
    "metadata_dict = metadata_df_copy.to_dict('index')\n",
    "# This eliminates need for filtering in subsequent operations\n",
    "print(f\"antall exp = {len(metadata_dict.keys())}\\nexcluded = {test_ids_to_exclude}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sensor Data\n",
    "# Constants\n",
    "HOST = '192.168.1.3'\n",
    "DATABASE = 'rnd'\n",
    "SELECTED_MACHINE = f'TS{tb_number}'  # Avoid hardcoding by fetching dynamically or using config\n",
    "\n",
    "# Use a context manager to handle the InfluxDB client connection\n",
    "with InfluxDBClient(host=HOST, database=DATABASE) as client:\n",
    "    # Fetch measurements and filter for the selected machine\n",
    "    measurements = client.query('SHOW MEASUREMENTS')\n",
    "    selected_machine = next((row['name'] for row in measurements.get_points() if row['name'] == SELECTED_MACHINE), None)\n",
    "\n",
    "    if not selected_machine:\n",
    "        raise ValueError(f\"Machine '{SELECTED_MACHINE}' not found in measurements.\")\n",
    "\n",
    "    # Fetch equipment and data tag values in a single query\n",
    "    tag_query = f'SHOW TAG VALUES FROM \"{SELECTED_MACHINE}\" WITH KEY IN (\"equipment\", \"data\")'\n",
    "    tag_results = client.query(tag_query)\n",
    "\n",
    "    # Extract equipment and data lists\n",
    "    equipment_list = [row['value'] for row in tag_results.get_points() if row['key'] == 'equipment']\n",
    "    data_list = [row['value'] for row in tag_results.get_points() if row['key'] == 'data']\n",
    "\n",
    "# Print results\n",
    "# print(\"Selected Machine:\", selected_machine)\n",
    "# print(\"Equipment List:\", equipment_list)\n",
    "# print(\"Data List:\", data_list)\n",
    "\n",
    "# for equipment in equipment_list:\n",
    "#      for data in data_list:\n",
    "#          print(equipment,data)\n",
    "\n",
    "# Hardcoded equipment and data lists\n",
    "f_equipment_list = ['G2101', 'FV2001', 'system', 'FT2301', 'TT2302', 'TT2308', 'FT2302']\n",
    "f_data_list = ['arc_voltage', 'arc_power', 'arc_current', 'flow', 'pressure', 'running', 'temperature']\n",
    "\n",
    "# Initialize a dictionary to store DataFrames for each Test ID\n",
    "sensor_dict = {}\n",
    "\n",
    "for Test_ID, excel in metadata_dict.items():\n",
    "    exp_start = excel['exp_start']\n",
    "    exp_stop = excel['exp_stop']\n",
    "    \n",
    "    # Initialize an empty list to store DataFrames for the current Test ID\n",
    "    dataframes = []\n",
    "    \n",
    "    # Iterate over equipment and data lists\n",
    "    for equipment in f_equipment_list:\n",
    "        for data in f_data_list:\n",
    "            query = f\"\"\"\n",
    "            SELECT mean(value) AS mean_value\n",
    "            FROM {selected_machine}\n",
    "            WHERE equipment = '{equipment}' AND data = '{data}'\n",
    "            AND time > '{exp_start.isoformat()}' AND time < '{exp_stop.isoformat()}'\n",
    "            GROUP BY time({lt}s, {lt*(-1)}s)\n",
    "            \"\"\"\n",
    "            data_points = client.query(query)\n",
    "            data_rows = list(data_points.get_points())\n",
    "            \n",
    "            if data_rows:\n",
    "                # Create a DataFrame for the current equipment and data\n",
    "                temp_df = pd.DataFrame({\n",
    "                    'timestamp': pd.to_datetime([dp['time'] for dp in data_rows]).tz_convert('Europe/Oslo'),\n",
    "                    f'{equipment}({data})': [dp['mean_value'] for dp in data_rows]\n",
    "                })\n",
    "\n",
    "                # Append the DataFrame to the list\n",
    "                dataframes.append(temp_df)\n",
    "    \n",
    "    # Merge all DataFrames for the current Test ID on the 'timestamp' column\n",
    "    if dataframes:\n",
    "        merged_sensor_df = reduce(lambda left, right: pd.merge(left, right, on='timestamp', how='outer'), dataframes)\n",
    "        # merged_sensor_df.set_index('timestamp', inplace=True)\n",
    "    else:\n",
    "        merged_sensor_df = pd.DataFrame()\n",
    "\n",
    "    merged_sensor_df = merged_sensor_df.interpolate()\n",
    "    merged_sensor_df = merged_sensor_df.dropna()\n",
    "    \n",
    "    # Store the merged DataFrame in the dictionary with Test ID as the key\n",
    "    sensor_dict[Test_ID] = merged_sensor_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handle missing numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp              0\n",
      "G2101(arc_voltage)     0\n",
      "G2101(arc_power)       0\n",
      "G2101(arc_current)     0\n",
      "G2101(running)         0\n",
      "G2101(temperature)     0\n",
      "FV2001(flow)           0\n",
      "FV2001(pressure)       0\n",
      "FV2001(temperature)    0\n",
      "system(running)        0\n",
      "FT2301(flow)           0\n",
      "FT2301(temperature)    0\n",
      "TT2302(temperature)    0\n",
      "TT2308(temperature)    0\n",
      "FT2302(flow)           0\n",
      "FT2302(temperature)    0\n"
     ]
    }
   ],
   "source": [
    "print(merged_sensor_df.isna().sum().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ftir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## FTIR Data\n",
    "# # Constants\n",
    "# HOST = '192.168.1.3'\n",
    "# DATABASE = 'instruments'\n",
    "# SELECTED_MACHINE = 'protea_ftir_2'  # Avoid hardcoding by fetching dynamically or using config\n",
    "\n",
    "# # Use a context manager to handle the InfluxDB client connection\n",
    "# with InfluxDBClient(host=HOST, database=DATABASE) as client:\n",
    "#     # Fetch measurements and filter for the selected machine\n",
    "#     measurements = client.query('SHOW MEASUREMENTS')\n",
    "#     selected_machine = next((row['name'] for row in measurements.get_points() if row['name'] == SELECTED_MACHINE), None)\n",
    "\n",
    "#     if not selected_machine:\n",
    "#         raise ValueError(f\"Machine '{SELECTED_MACHINE}' not found in measurements.\")\n",
    "\n",
    "#     # Fetch equipment and data tag values in a single query\n",
    "#     tag_query = f'SHOW TAG VALUES FROM \"{SELECTED_MACHINE}\" WITH KEY IN (\"equipment\", \"data\")'\n",
    "#     tag_results = client.query(tag_query)\n",
    "\n",
    "#     # Extract equipment and data lists\n",
    "#     equipment_list = [row['value'] for row in tag_results.get_points() if row['key'] == 'equipment']\n",
    "#     data_list = [row['value'] for row in tag_results.get_points() if row['key'] == 'data']\n",
    "\n",
    "# # # Print results\n",
    "# # print(\"Selected Machine:\", selected_machine)\n",
    "# # print(\"Equipment List:\", equipment_list)\n",
    "# # print(\"Data List:\", data_list)\n",
    "\n",
    "# # for equipment in equipment_list:\n",
    "# #      for data in data_list:\n",
    "# #          print(equipment,data)\n",
    "\n",
    "# # Hardcoded equipment and data lists\n",
    "# f_equipment_list = ['Channel_2']\n",
    "# f_data_list = ['NO_Corrected', 'NO2_Corrected']\n",
    "\n",
    "# # Initialize a dictionary to store DataFrames for each Test ID\n",
    "# ftir_dict = {}\n",
    "\n",
    "# for Test_ID, excel in metadata_dict.items():\n",
    "#     exp_start = excel['exp_start']\n",
    "#     exp_stop = excel['exp_stop']\n",
    "    \n",
    "#     # Initialize an empty list to store DataFrames for the current Test ID\n",
    "#     dataframes = []\n",
    "    \n",
    "#     # Iterate over equipment and data lists\n",
    "#     for equipment in f_equipment_list:\n",
    "#         for data in f_data_list:\n",
    "#             query = f\"\"\"\n",
    "#             SELECT mean(value) AS mean_value\n",
    "#             FROM {selected_machine}\n",
    "#             WHERE equipment = '{equipment}' AND data = '{data}'\n",
    "#             AND time > '{exp_start.isoformat()}' AND time < '{exp_stop.isoformat()}'\n",
    "#             GROUP BY time(30s, -30s)\n",
    "#             \"\"\"\n",
    "#             data_points = client.query(query)\n",
    "#             data_rows = list(data_points.get_points())\n",
    "            \n",
    "#             if data_rows:\n",
    "#                 # Create a DataFrame for the current equipment and data\n",
    "#                 temp_df = pd.DataFrame({\n",
    "#                     'timestamp': pd.to_datetime([dp['time'] for dp in data_rows]).tz_convert('Europe/Oslo'),\n",
    "#                     f'{equipment}({data})': [dp['mean_value'] for dp in data_rows]\n",
    "#                 })\n",
    "\n",
    "#                 # Append the DataFrame to the list\n",
    "#                 dataframes.append(temp_df)\n",
    "    \n",
    "#     # Merge all DataFrames for the current Test ID on the 'timestamp' column\n",
    "#     if dataframes:\n",
    "#         merged_ftir_df = reduce(lambda left, right: pd.merge(left, right, on='timestamp', how='outer'), dataframes)\n",
    "#         # merged_ftir_df.set_index('timestamp', inplace=True)\n",
    "#     else:\n",
    "#         merged_ftir_df = pd.DataFrame()\n",
    "    \n",
    "#     # Store the merged DataFrame in the dictionary with Test ID as the key\n",
    "#     ftir_dict[Test_ID] = merged_ftir_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Sensor and FTIR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp', 'G2101(arc_voltage)', 'G2101(arc_power)',\n",
      "       'G2101(arc_current)', 'G2101(running)', 'G2101(temperature)',\n",
      "       'FV2001(flow)', 'FV2001(pressure)', 'FV2001(temperature)',\n",
      "       'system(running)', 'FT2301(flow)', 'FT2301(temperature)',\n",
      "       'TT2302(temperature)', 'TT2308(temperature)', 'FT2302(flow)',\n",
      "       'FT2302(temperature)'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "## Manipulations\n",
    "# Check column names in sensor_dict\n",
    "print(sensor_dict[Test_ID].columns)\n",
    "# Check column names in ftir_dict\n",
    "# print(ftir_dict[Test_ID].columns)\n",
    "\n",
    "merged_dict = {}\n",
    "\n",
    "for Test_ID in sensor_dict.keys():\n",
    "    # Merge the DataFrames from both dictionaries on the 'timestamp' column\n",
    "    # influx_df = pd.merge(sensor_dict[Test_ID], ftir_dict[Test_ID], on='timestamp', how='outer')\n",
    "    influx_df = pd.DataFrame(sensor_dict[Test_ID])\n",
    "    # influx_df = influx_df.query('`G2101(arc_power)` > 15 & `G2101(arc_voltage)` > 500 & `FV2001(flow)` > 25')\n",
    "    influx_df = influx_df.query('`system(running)` == 1')\n",
    "\n",
    "    # influx_df['NO/NO2'] = influx_df['Channel_2(NO_Corrected)'] / influx_df['Channel_2(NO2_Corrected)']\n",
    "    # influx_df['NOx%'] = influx_df['Channel_2(NO_Corrected)'] + influx_df['Channel_2(NO2_Corrected)']\n",
    "    # std_temp = 273\n",
    "    # std_pressure = 101325\n",
    "    # Mol_mas_N = 14\n",
    "    # R = 8.3145\n",
    "    # influx_df['EC_calc'] = (100000 * std_temp * R * influx_df['G2101(arc_power)']) / (std_pressure * Mol_mas_N * influx_df['FV2001(flow)'] * influx_df['NOx%'])\n",
    "    \n",
    "    influx_df['enthalpy_calc'] = influx_df['G2101(arc_power)'] / influx_df['FV2001(flow)']\n",
    "\n",
    "    # Store the merged DataFrame in the dictionary with Test ID as the key\n",
    "    merged_dict[Test_ID] = influx_df\n",
    "\n",
    "merged_influx_df = pd.concat(merged_dict.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Statistical Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reformatted Statistics DataFrame:\n"
     ]
    }
   ],
   "source": [
    "stat_dict = {}\n",
    "\n",
    "# Iterate over the keys (Test_IDs)\n",
    "for Test_ID in merged_dict.keys():\n",
    "    # merged_df = pd.merge(merged_dict[Test_ID], ftir_dict[Test_ID], on='timestamp', how='outer')\n",
    "    merged_df = merged_dict[Test_ID]\n",
    "    # merged_df['NO/NO2']=merged_df['Channel_2(NO_Corrected)']/merged_df['Channel_2(NO2_Corrected)']\n",
    "    # merged_df['NOx%']=merged_df['Channel_2(NO_Corrected)']+merged_df['Channel_2(NO2_Corrected)']\n",
    "    \n",
    "    # std_temp = 273\n",
    "    # std_pressure = 101325\n",
    "    # Mol_mas_N = 14\n",
    "    # R = 8.3145\n",
    "    # merged_df['EC_calc'] = (100000*std_temp*R*merged_df['G2101(arc_power)']) / (std_pressure*Mol_mas_N*merged_df['FV2001(flow)']*merged_df['NOx%'])\n",
    "    merged_df['enthalpy_calc'] = merged_df['G2101(arc_power)'] / merged_df['FV2001(flow)']\n",
    "\n",
    "    # Exclude the 'timestamp' column from calculations\n",
    "    data_columns = merged_df.columns.difference(['timestamp'])\n",
    "    mean_values = merged_df[data_columns].mean()\n",
    "    std_values = merged_df[data_columns].std()\n",
    "    cv_values = std_values / mean_values\n",
    "\n",
    "    # Store the results in a DataFrame\n",
    "    stats = {'mean': mean_values, 'std': std_values, 'cv': cv_values}\n",
    "    stat_df = pd.DataFrame(stats).transpose()\n",
    "    stat_df['Test_ID'] = Test_ID\n",
    "    stat_dict[Test_ID] = stat_df\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "all_stats_df = pd.concat(stat_dict.values(), ignore_index=False)\n",
    "\n",
    "stat_df = pd.DataFrame()\n",
    "# For each parameter, append mean, std, cv in order\n",
    "for parameter in data_columns:\n",
    "    stat_df = pd.concat(\n",
    "        [stat_df, \n",
    "         all_stats_df.loc['mean', [parameter]].reset_index(drop=True).rename(columns={parameter: parameter + '_mean'}),\n",
    "         all_stats_df.loc['std', [parameter]].reset_index(drop=True).rename(columns={parameter: parameter + '_std'}),\n",
    "         all_stats_df.loc['cv', [parameter]].reset_index(drop=True).rename(columns={parameter: parameter + '_cv'})],\n",
    "        axis=1\n",
    "    )\n",
    "# Add Test_ID as a column\n",
    "stat_df['Test_ID'] = all_stats_df.iloc[::3, all_stats_df.columns.get_loc('Test_ID')].to_list()\n",
    "\n",
    "print(\"Reformatted Statistics DataFrame:\")\n",
    "# stat_df.tail(10)\n",
    "# print(stat_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Influx and excel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(metadata_df_copy, stat_df, on='Test_ID')\n",
    "# final_df = final_df.drop(columns=[col for col in final_df.columns if 'cv' in col])\n",
    "# final_df.to_csv('final_df.csv')\n",
    "\n",
    "# id = final_df[final_df['Test_ID']=='2025-exp30']\n",
    "# print('exp 30 energy cost =', id['EC_calc_mean'].to_string(index=False))\n",
    "\n",
    "# final_df.isna().sum()\n",
    "\n",
    "# final_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save as pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_influx_df.to_pickle(f'TB{tb_number}_merged_influx_df.pkl')\n",
    "stat_df.to_pickle(f'TB{tb_number}_stat_df.pkl')\n",
    "final_df.to_pickle(f'TB{tb_number}_final_df.pkl')\n",
    "\n",
    "with open(f'TB{tb_number}_merged_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(merged_dict, file)\n",
    "# Save sensor_dict as a pickle file\n",
    "with open(f'TB{tb_number}_sensor_dict.pkl', 'wb') as file:\n",
    "    pickle.dump(sensor_dict, file)\n",
    "# Save ftir_dict as a pickle file\n",
    "# with open(f'TB{tb_number}_ftir_dict.pkl', 'wb') as file:\n",
    "#     pickle.dump(ftir_dict, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
